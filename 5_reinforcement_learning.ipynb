{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning\n",
    "\n",
    "In this practical we are going to focus on a particular flavor of reinforcement learning called __temporal difference learning__. \n",
    "\n",
    "## Part 1 - Temporal difference learning\n",
    "\n",
    "We begin with the simplest version of temporal difference learning, known as __TD(0)__. During the lecture, you saw that the quantity we are trying to learn is the _discounted sum of all future rewards_ $V(t)$, which is given by\n",
    "\n",
    "\\begin{equation}\n",
    "V(t) = \\mathbb{E}[\\gamma^0r(t)+\\gamma^1r(t+1)+\\gamma^2r(t+2)\\ldots]\n",
    "\\end{equation}\n",
    "\n",
    "Where $\\mathbb{E}$ indicates the expected value which is taken over the rewards $r(\\cdot)$ at all future times, discounted by a factor $0 < \\gamma < 1$.\n",
    "\n",
    "In this tutorial, we are going to consider a very simple example in which we have a few spatial locations that our learning agent can be in, so we are going to use the subscript $V_x(t)$, which represents the expectation of the discounted sum of all future rewards given that at time $t$ we are in position $x$. We will use $x=1, 2, 3, \\ldots$ to denote our spatial locations. \n",
    "\n",
    "Generally, in a novel environment (I talk about environment here as an example, but obviously this algorithm is more general) we do not know a good value function $V_x(t)$. We probably have some expectations of where we could get rewards for instance, but it's not until we start exploring the environment that we can update our value function and learn a useful model of the world. Temporal difference learning is simply a way in which we can _continuously_ update our value function, our model of the world. We will see that the way temporal difference allows us to do this is very much in line with our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "In general, the value function $V_x(t)$ is not known, and the point of temporal difference learning is exactly to learn a good value function. Can you explain in your own words why an agent (such as an animal exploring an environment) would be interested in learning a good $V_x(t)$? What does it mean for a certain location $x$ to have a high/low $V_x(t)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the following scenario: we are an agent in an environment consisting of three aligned spatial locations, and we are situated at the leftmost location ($x=1$). We suppose that at each time step we make a move to the right. At every location, there is some reward, at location 2 we have $R_2=3$ and at location 3 we have $R_3=2$. We also have a discount factor $\\gamma=0.5$, which means that a reward that we get in the next time step is half as valuable as the same reward if we would get it now. Note that we use the following (and perhaps slightly counterintuitive) convention throughout the assignment: if at time $t$ we make a move to a location $x$ where we get reward $R_x$, then $r(t)=R_x$, i.e. the reward of the current time $r(t)$ is the reward that we get by making the move that we make at time $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/rl_1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Write down the value function $V_1(t)$ given our current knowledge of the environment.\n",
    "_Hint:_ You know that at time $t$ you are going to take one step to the right, and similarly at time $t+1$. After that, the exploration ends. You can use this information to compute $V(t)$ exactly using the formula we wrote down at the beginning of the notebook. You can get rid of the expectation sign in the equation: we assume we move deterministically in the environment and we know exactly what the rewards are, so there is no uncertainty anymore that we have to handle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Suppose that we have made our first move, and we are now at location $x=2$, what is our current value function $V_2(t+1)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a step back (in our reasoning, not in our simulation). I argue that, if we had a perfect value function (a perfect model of the environment in this case), then\n",
    "\n",
    "\\begin{equation}\n",
    "V_{x_t}(t) = r(t) + \\gamma V_{x_{t+1}}(t+1)\n",
    "\\end{equation}\n",
    "\n",
    "That is, if we have a perfect model, the discounted sum of all future rewards is simply the reward that we are going to take in this time step $t$, plus the discounted sum of all future rewards from there on out (the value function at the new location $x_{t+1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Write down the expression for $V_1(t)$ using the equation above. Convince yourself that this makes sense. \n",
    "\n",
    "_Hint_: you should get the same result you got in Question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's return to our simulation. We are at time $t$, and have made a move from $x=1$ to $x=2$, and we are ready to collect are well-deserved reward (after all, we managed to move a whole step from one imaginary box to an adjacent and equally imaginary box). But what a terrible surprise awaits us! The reward at this location is not $R_2=3$ as we thought, we find that it is actually $R_2=1$! Thoroughly and rightfully disappointed, we proceed to gather the reward. But as we mildly enjoy this meager consolation, we think back to our expectations, in particular we look back at the bright future of copious rewards we envisioned when we computed $V_1(t)$, in the previous time step. What fools we have been!\n",
    "\n",
    "![title](images/rl_2.svg)\n",
    "\n",
    "Let's denote our wrong expectation of the reward at location 2 as $\\hat{R}_2$ (yes, we could have been more precise and always write $\\mathbb{E[R_2]}$ instead of $R_2$, because in fact we were using our best estimate of $R_2$ - and not $R_2$ itself, which we don't know - in our calculation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "If you were to translate your disappointment for the lower reward you received at $x=2$ into an update of your value function, how would you intuitively modify $V_1(t)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously discussed that, with a perfect value function,\n",
    "\n",
    "\\begin{equation}\n",
    "V_{x_t}(t) = r(t) + \\gamma V_{x_{t+1}}(t+1)\n",
    "\\end{equation}\n",
    "\n",
    "However, in our case, our expectations turned out to be wrong, because we were misinformed about the reward at location 2. That is \n",
    "\n",
    "\\begin{equation}\n",
    "V_{1}(t) = \\hat{R}_2 + \\gamma V_2(t+1) \\neq R_2 + \\gamma V_2(t+1)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "In the slides, you saw how to compute the _error in reward prediction_ $\\delta(t)$. Compute $\\delta_1(t)$ for the present case (remember that we add the spatial subscript here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 6\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can update our value function at $x=1$ using\n",
    "\\begin{equation}\n",
    "V_1(t+1) = V_1(t) + \\alpha \\delta_1(t)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Calculate the new value function $V_1(t+1)$ using a learning rate $\\alpha=0.1$. What does this mean for our evaluation of how good it is to be a location 1? Does this correspond with your intuition on how you would have updated your value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 7\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did a fair amount of pondering, as we were hanging out at $x=2$, on how to change our value function for $x=1$ given our disappointing discovery. But as we are ready to carelessly stroll on towards $x=3$, we notice something unexpected: from $x=2$ we can take a secret passage and access a fourth imaginary box, which contains $R_4=10$ reward! We decide immediately that in the time step $t+1$ we are not going to go to $R_3$, which only contains a small reward, but instead we are going to take the secret passage and collect the bounty at $x=4$. \n",
    "\n",
    "![title](images/rl_3.svg)\n",
    "\n",
    "But this means that once again we have to revisit our judgment about $V_1$! In fact, while we thought that the sum of our expected rewards was $\\hat{R}_2 + \\gamma R_3$, it turns out that it's in fact $ R_2 + \\gamma R_4$. (Again, we are being slightly simplistic with the math: instead of taking a sum over all future paths weighted by probability of taking that path, we simply assume that once we discover the new path we are going to always take it with probability one, and this allows us to write the simpler expression $R_2 + \\gamma R_4$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Write down the final $\\delta_1(t)$ given this new discovery, and update $V_1$ accordingly. Does this result correspond to how you would intuitively change the value of $V_1$ given that you can now get a high reward at $x=4$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 8\n",
    "\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part 2 - Eligibility traces: how remembering helps learning\n",
    "\n",
    "We are now going to extend slightly beyond what you saw in the lectures. At the beginning of this practical, I told you that this algorithm is known as TD(0), where TD clearly stands for temporal difference. But what does the zero stand for? \n",
    "\n",
    "Before we continue, take a moment to realize that although temporal difference learning is a reinforcement learning algorithm, _we are not actually learning a policy for our agent_: what we have learned is a value function for our environment. Our algorithm is not telling us where to go, it is just telling us how good it is to be in a place rather than another. Other algorithms, such as SARSA, explicitly deal with policies.  \n",
    "\n",
    "Let's consider a new maze, this time with a few more locations, as shown in the picture. There is a fork in the road, and there is a reward waiting for us on one of the arms of the maze, but we do not know that there is reward or on which arm it is. We suppose that we move in the environment with a policy whereby we always go to the accessible location with the highest value (and roll a dice if all options have the same value). Ideally, _what we want to learn is which side is the profitable one_, such that next time we are the fork in the road we can make the correct choice and earn the reward. \n",
    "\n",
    "![title](images/rl_4.svg)\n",
    "\n",
    "\n",
    "We imagine that we are strolling around and we find ourselves at $x=3$ (a nice beginning _in medias res_). As we do not have any expectations of reward, our value is \n",
    "\n",
    "\\begin{align}\n",
    "V_3(t) &= r(t) + \\gamma V_{x_{t+1}}(t+1) \\\\\n",
    "&= \\hat{R}_4 + \\gamma V_4(t+1) \\\\\n",
    "&= \\hat{R}_4\\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "where $\\hat{R}_4$ is again what we expect to find at $x=4$. Instead, we find that there is quite some reward at $x=4$. Happily surprised, we compute\n",
    "\n",
    "\\begin{align}\n",
    "\\delta_3(t) &= R_4 + \\gamma V_4(t+1) - \\hat{R}_4 - \\gamma V_4(t+1)  \\\\\n",
    "&= R_4 - \\hat{R}_4 \\\\\n",
    "&= 5\n",
    "\\end{align}\n",
    "\n",
    "Thus, with a positive $\\delta_x(t)$, we increase the value function at $x=3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "The update to the value function I just illustrated is exactly what we did in the first part of the assignment. However, in this larger maze, one could argue that that this is quite unsatisfactory. Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you lose all faith in temporal difference learning, I should tell you that we can do better, and this is where the zero in TD(0) comes in. The main idea is that we can remember some of the previous locations we have visited (or actions we have performed), so that when we learn something new at $t+1$ we can update our beliefs about those locations as well. This is done by equipping each location with an __eligibility trace__:\n",
    "\n",
    "\\begin{equation}\n",
    "e_x(t) = \\left\\{\\begin{matrix}\n",
    "\\gamma \\lambda e_x(t-1) & \\mathrm{if} \\, x \\neq x^*\\\\ \n",
    "\\gamma \\lambda e_x(t-1) + 1 &  \\mathrm{if}\\, x=x^*\n",
    "\\end{matrix}\\right.\n",
    "\\end{equation}\n",
    "\n",
    "Here I used $x^*$ to denote our current location. We also have the parameter $0<\\lambda<1$ which is called __trace decay__, and this is what is actually inside the brackets of TD(0) In fact, the more general algorithm is known as TD($\\lambda$), where $\\lambda=0$ is just the special case where we do not keep any traces. Let's break this down: every location $x$ gets its eligibility trace $e_x(t)$. As we move around, as long as we don't visit a certain location $x$, its trace just keeps dying out (every iteration we multiply its past value it with $\\lambda$ which is between 0 and 1). Whenever we visit location $x$, its trace gets bumped up by one. For an example location $x=2$, the trace may look something like this:\n",
    "\n",
    "![title](images/rl_5.svg)\n",
    "\n",
    "Notice that as we move away from a location, the trace of that location decays, while every time we visit it, it becomes more relevant again. So suppose we are at the current location $x=x^*$ and have computed $\\delta_{x^*}(t)$: we can now update the value function at _all_ locations using.\n",
    "\n",
    "\\begin{equation}\n",
    "V_x(t+1) = V_x(t) + \\alpha \\delta_{x^*}(t) e_x(t)\n",
    "\\end{equation}\n",
    "\n",
    "Which looks just like the learning equation we have seen before, except that we _scale our update by the eligiblity trace_. Thus, for a location that we visited recently, we are going to change our value function quite a bit, whereas locations that we visited a long time ago will not get updated much, because they are not really relevant for what is happening at the present location. \n",
    "\n",
    "Let's now suppose we took the following route: we start at $x=0$ at $t=0$. Then we move as follows: $0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3 \\rightarrow 4$. We start out with $e_x(t=0)=0$. \n",
    "\n",
    "![title](images/rl_6.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Compute the eligibility trace for $x=2$ and $\\lambda=0.5$ along our route (use the same discount factor $\\gamma=0.5$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Suppose that at $t=3$ we compute $\\delta_{x^*=3}(t=3)=5$. What is $V_2(t=4)$ going to be? (Suppose $V_2(3)=0$ and remember, $\\alpha=0.1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Have we succeeded in the goal of learning which arm of the maze to take? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "[your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 (bonus)\n",
    "We said that in TD(0) only the current location (or immediately previous, depending on how you interpret it) gets updated. But in the end, I guarantee you that we can converge to the correct value function for our whole maze by using TD(0). Can you think how that could work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful links, which inspired this tutorial:\n",
    "- http://www.scholarpedia.org/article/Temporal_difference_learning\n",
    "- https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
