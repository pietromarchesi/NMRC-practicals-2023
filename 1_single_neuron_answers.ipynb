{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modeling a single neuron\n",
    "\n",
    "In this first tutorial, we are going to explore a very simple computational model for the neuron. In spite of its simplicity, this model is widely used in artificial neural networks. In particular, we are going to explore the characteristics of different activation functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "In the lecture, you saw the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "a_i = f \\left( \\sum_{j=1}^N w_{ij}a_j-b_i \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Which aspects of the biological neuron are being modeled by $a_j$, $w_{ij}$, $b_i$, $f$, and $a_i$? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 1\n",
    "$w_{ij}$: Weight, so the strength of the synapses j to neuron i <br>\n",
    "$b_{i}$: Bias, a value which determines how quickly a neuron will respond to particular inputs.<br>\n",
    "$f$: Transfer function, so the transduction of the incoming input to the output of the neuron. <br>\n",
    "$a_i$: Output of the neuron i, in firing rate.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Which physiological aspects of real (biological) neurons are missing in this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 2\n",
    "Several aspects of real neurons are missing in this model, because this model is a gross oversimplification. \n",
    "Some examples: <br>\n",
    "\n",
    "Timing of spikes, there is no time component in this model.<br>\n",
    "Abundancy of dendrites / axon branches and their spatial arrangement.<br>\n",
    "Refractory period.<br>\n",
    "Adaptation, plasticity.<br>\n",
    "No biomolecular effects such as aging, hormones, disease etc.<br>\n",
    "A biologically plausible transfer function; a hardlimit transfer function is not possible in real life.<br>\n",
    "Effects of LTP.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a neuron, which receives input from a single presynaptic neuron. We will now drop the subscripts and call the output of the neuron of interest $a$, the output of the presynaptic neuron $p$, and the bias $b$.\n",
    "\n",
    "We are interested in understanding how the output of our neuron behaves as a function of the input. We begin by writing up a short function that, for a given weight $w$, input $p$, and bias $b$,  and for a given transfer function, will compute the output of our neuron and plot it. In the cell below is a chunk of Python code, where I import the function `plot_single_neuron_output`, which we will be using throughout the tutorial. Make sure you run the cell below before continuing, so that the function become available when we want to call it later on in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils_assignment_1 import plot_single_neuron_output\n",
    "from ipywidgets import interact, fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function in an interactive way, so we can nicely visualize the effect of changing parameters. For now we consider two settings, namely $w=1$ and $w=-1$, and we keep $b$ fixed at $b=0.5$. We choose the hard-limit function (`hardlim`) to start with. Run the cell below: you will see two sliders, with which you can set $w$ to either 1 or $-1$ and vary $p$ between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b1125dde91403580978f6223081ba1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_single_neuron_output, w=(-1, 1, 2), p=(0.0, 1.0, 0.05), b=fixed(0.5), tf_name=fixed('hardlim'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Describe how output $a$ (the horizontal red line) varies as a function of $p$ (where $p$ is between 0 and 1), for both cases $w=1$ and $w=-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "$w=1$: This neuron simply multiplied the input $p$ by 1, thus doesn't change it. If this reached threshold $b = 0.5$ it fires, i.e. the output $a = 1$<br>\n",
    "$w=-1$: This neuron received negative input from the negative $w$, so this can be seen as inhibitory input. With $p$ between 0 and 1, it will never reach threshold and spike (i.e. never reach $a = 1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume now that the input gets a larger dynamic range and can vary between -4 and 4. Run the cell below and use the slider to set $w$ vary the value of $p$ in this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430eeef83205403da64d493d1465abec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_single_neuron_output, w=(-1, 1, 2), p=(-4.0, 4.0, 0.05), b=fixed(0.5), tf_name=fixed('hardlim'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "Describe again how the output $a$ varies as a function of $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 4\n",
    "When $p$ is allowed to vary from -4 to 4:<br>\n",
    "$w=1$: As before, the in input $p$ is multiplied by 1, thus stays the same. Ergo, when $p$ reached threshold $b = 0.5$ the cell will spike, as in $a = 1$.<br>\n",
    "$w=-1$: The input is multiplied by $w = -1$. In this case, a negative input will be able to reach threshold, i.e. $p$ < $-0.5$. Other inputs will not reach threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explore the effect of $b$ on the output. We further assume that we receive a binary signal as input ($p$ is either 0 or 1). Run the cell below, and use the slider to vary the value of $b$. Understand how that affects the transfer function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f7285075e44fe1b9fe2c63d945ab97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_single_neuron_output, w=(-1, 1, 2), p=(0, 1, 1), b=(-2, 2, 0.05), tf_name=fixed('hardlim'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "We again study both cases $w=1$ and $w=-1$. For what ranges of the bias $b$ will our neuron be able to discriminate a binary input (which takes on either 1 or 0) coming from the presynpatic cell?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 5\n",
    "$w=1$: Here the best classification between 1 and 0 is the middle, i.e. $b=0.5$. As the weight won't affect the $p$ value, inputs of 0 will be < $b = 0.5$, and inputs of 1 > $b$, giving appropriate outputs. If the input will only be 0 or 1, $b$ should be in the range $0 \\leq b < 1$.<br>\n",
    "$w=-1$: When the input is multiplied by $-1$, it's basically the same but negative, ergo $b$ should be in the range $-1 \\leq b < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer functions can have different ranges (codomain). You can take a look at this [Wikipedia entry](https://en.wikipedia.org/wiki/Activation_function) for a review of the topic and a list of popular functions. Importantly, while the output of the `hardlim` function is between 0 and 1, that need not be the case. Here we introduce two new functions.\n",
    "\\begin{align}\n",
    "\\text{Linear function:}\\quad & \\texttt{purelin(x)} & \\rightarrow & \\quad f(x)=x \\\\\n",
    "\\text{Hyperbolic tangent:} \\quad & \\texttt{tanh(x)} &\\rightarrow & \\quad f(x)=\\frac{e^{2x}-1}{e^{2x}+1}\n",
    "\\end{align}\n",
    "Note that the codomain of the linear function is $(-\\infty, \\infty)$, while for the hyperbolic tangent it is $(-1, 1)$. Now, run the cell below, use the drop down menu to select different transfer functions, and the slider to change the value of the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbe3458b15c34363a9cc8a8586de64dd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(plot_single_neuron_output, w=(-1, 1, 2), p=(0, 1, 1), b=(-2, 2, 0.05), tf_name=['hardlim', 'hardlims', 'purelin', 'tanh']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "What is the common effect of changing the bias on the output of all the transfer functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 6\n",
    "It shifts the transfer function along the input axes (in this case the $x$-axis). The shape of the functions remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "Which configuration of transfer function and parameters classifies values of the input $p$ most strictly into 'categories'? We refer to transfer functions that make such a strong distinction between two ranges of input values as _classifiers_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 7\n",
    "To make a strong classifier, the hardlimit transfer function would be ideal. The values of $b$ and $w$ will depend on what you want to classify, but the $w$ can't be 0. Dependent on your input $p$, your threshold should be of a logical value, i.e. inputs and weight of 1 and a threshold of 500 will not classify. \n",
    "<br>\n",
    "For instance, as seen in previous questions, classifying binary inputs to binary outputs (a really boring classification), can be readily done with $0 \\leq b < 1$, and a $w = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Conversely, which configuration is the worst at grouping values of the input in separate categories? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 8\n",
    "A linear transfer function (like purelin) will not give strict classifications, but a continous output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "What could be the advantage of a poorly classifying configuration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 9\n",
    "This will be an estimator. If you want your output to reflect the input in a  continous, linear fashion, you would need this configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Which transfer function is the most biological one? Explain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 10\n",
    "Of our transfer functions the hyperbolic tangend (tanh) function is the most biologically plausible one. These shapes of curves are often seen in biology and psychology (for those interested, google psychometric curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "Consider the transfer function that you gave as an answer in the previous question. Go back to the last plot, and set it up using $w=1$ and $b=0$. In which input range does the transfer function behave as an estimator of the input, and in which range as a classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 11\n",
    "Estimator range: The middle of the tansig curve is relatively straight, and acts as an estimator, much like a linear transfer function.<br>\n",
    "Classifier range: The ends of the tansig curve plateau towards 0 and 1, and this acts similar to the hard limit transfer function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "What is the essential mathematical feature of a transfer function aimed at classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 12\n",
    "The feature of non-lineararity; it needs a relatively strict distinction between values, such as we see in the hard limit function. This is highly non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "Which aspects of how neurons render the transfer from synaptic input to output can be modeled with a sigmoid function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 13\n",
    "The sigmoid function has a threshold-like point, it has to reach certain input to start responding. Moverover, in the middle, a small increase in input gives a large increase in output (i.e. it has a dynamic range). furthermore, the function also has a maximum, where the output doesn't increase further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "Which physiological aspects of real neurons can be seen as the counterpart of the bias in computational neurons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 14\n",
    "This can be thought of as the resting potential of a neurons, spontaneous input of other neurons on said neuron, biomolecular effects (i.e. hormones etc), "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
